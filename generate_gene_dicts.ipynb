{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "import gzip, shutil, pickle\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c52cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PbergheiANKA', 'PblacklockiG01', 'PmalariaeUG01', 'PvinckeibrucechwattiDA', 'PreichenowiCDC', 'PcynomolgiM', 'PovalecurtisiGH01', 'PovalewallikeriPowCR01', 'Pfalciparum3D7', 'PgaboniG01', 'PknowlesiH', 'PadleriG01', 'Pgallinaceum8A', 'PvivaxSal1', 'Pchabaudichabaudi', 'PinuiSanAntonio1', 'PfragileNilgiri', 'PcoatneyiHackeri', 'Pyoeliiyoelii17XNL2023'}\n",
      "['PadleriG01', 'PbergheiANKA', 'PblacklockiG01', 'Pchabaudichabaudi', 'PcoatneyiHackeri', 'PcynomolgiM', 'Pfalciparum3D7', 'PfragileNilgiri', 'PgaboniG01', 'Pgallinaceum8A', 'PinuiSanAntonio1', 'PknowlesiH', 'PmalariaeUG01', 'PovalecurtisiGH01', 'PovalewallikeriPowCR01', 'PreichenowiCDC', 'PvinckeibrucechwattiDA', 'PvivaxSal1', 'Pyoeliiyoelii17XNL2023']\n"
     ]
    }
   ],
   "source": [
    "fasta_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "fasta_files = list(fasta_dir.glob(\"*.fasta\"))\n",
    "\n",
    "genomes = { f.name.split('_')[1] for f in fasta_files }\n",
    "\n",
    "genome_list = sorted(genomes)\n",
    "\n",
    "print(genomes)       # e.g. {'PbergheiANKA', 'PvivaxSal1', ...}\n",
    "print(genome_list)   # e.g. ['PbergheiANKA', 'PblacklockiG01', ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c637f080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(genome_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74784198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "\n",
    "species_to_fasta = {}\n",
    "for sp in genome_list:\n",
    "    fn = f\"PlasmoDB-68_{sp}_AnnotatedProteins.fasta\"\n",
    "    fp = fasta_dir / fn\n",
    "    if fp.exists():\n",
    "        species_to_fasta[sp] = fp\n",
    "    else:\n",
    "        matches = list(fasta_dir.glob(f\"*{sp}*AnnotatedProteins*.fasta\"))\n",
    "        if len(matches) == 1:\n",
    "            species_to_fasta[sp] = matches[0]\n",
    "        elif len(matches) > 1:\n",
    "            print(f\"multiple matches for {sp}: {matches}\")\n",
    "        else:\n",
    "            print(f\"no file found for {sp}\")\n",
    "\n",
    "for sp, fasta_path in species_to_fasta.items():\n",
    "    print(f\"{sp} ‚Üí {fasta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = Path(\"genomes_to_annotate_with_PlasmoFP/annotated_fasta\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for species, fasta_path in species_to_fasta.items():\n",
    "    dest_path = dest_dir / fasta_path.name\n",
    "    try:\n",
    "        shutil.copy2(fasta_path, dest_path)\n",
    "        print(f\"{species}: copied to {dest_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{species}: source file not found ({fasta_path})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{species}: failed to copy ({e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path(\"genomes_to_annotate_with_PlasmoFP/annotated_fasta\")\n",
    "out_dir = Path(\"genomes_to_annotate_with_PlasmoFP/1_length_filtered_fastas\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fasta_fp in src_dir.glob(\"*.fasta\"):\n",
    "    filtered_records = (\n",
    "        rec for rec in SeqIO.parse(fasta_fp, \"fasta\")\n",
    "        if len(rec.seq) <= 1200\n",
    "    )\n",
    "    out_fp = out_dir / (fasta_fp.stem + \"_filtered.fasta\")\n",
    "    count = SeqIO.write(filtered_records, out_fp, \"fasta\")\n",
    "    print(f\"{fasta_fp.name}: wrote {count} records ‚â§1200 aa ‚Üí {out_fp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c95b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path(\"genomes_to_annotate_with_PlasmoFP/1_length_filtered_fastas\")\n",
    "out_dir = Path(\"genomes_to_annotate_with_PlasmoFP/2_multi_step_filtered_fastas\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "keywords = [\n",
    "    \"emp1\", \"merozoite surface protein\", \"msp\",\n",
    "    \"rifin\", \"stevor\", \"fragment\",\n",
    "    \"vir\", \"variant interspersed repeat\", \"variant interspersed repeats\",\n",
    "    \"fam-\",    # fam-a/b/c...\n",
    "    \"pir\",     # PIR/CIR\n",
    "    \"cir\",\n",
    "    \"yir\",     # YIR family\n",
    "    \"surf\",    # SURFIN\n",
    "]\n",
    "\n",
    "def filter_counts(fasta_fp):\n",
    "    counts = {\n",
    "        \"total\": 0,\n",
    "        \"step1_non_pseudo\": 0,\n",
    "        \"step2_iso1\": 0,\n",
    "        \"step4_no_keyword\": 0,\n",
    "        \"step5_no_api\": 0,\n",
    "    }\n",
    "    for rec in SeqIO.parse(fasta_fp, \"fasta\"):\n",
    "        counts[\"total\"] += 1\n",
    "        desc = rec.description.lower()\n",
    "\n",
    "        if \"is_pseudo=false\" not in desc:\n",
    "            continue\n",
    "        counts[\"step1_non_pseudo\"] += 1\n",
    "\n",
    "        if \"transcript=\" not in desc:\n",
    "            continue\n",
    "        trans_id = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "        if not (trans_id.endswith(\".1\") or trans_id.endswith(\"_1\") or trans_id.endswith(\"_t1\")):\n",
    "            continue\n",
    "        counts[\"step2_iso1\"] += 1\n",
    "\n",
    "        if any(kw in desc for kw in keywords):\n",
    "            continue\n",
    "        counts[\"step4_no_keyword\"] += 1\n",
    "\n",
    "        gene_parts = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "        if gene_parts and \"api\" in gene_parts[0].lower():\n",
    "            continue\n",
    "        counts[\"step5_no_api\"] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def keep_record(rec):\n",
    "    desc = rec.description.lower()\n",
    "    if \"is_pseudo=false\" not in desc:\n",
    "        return False\n",
    "    if \"transcript=\" not in desc:\n",
    "        return False\n",
    "    trans_id = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "    if not (trans_id.endswith(\".1\") or trans_id.endswith(\"_1\") or trans_id.endswith(\"_t1\")):\n",
    "        return False\n",
    "    if any(kw in desc for kw in keywords):\n",
    "        return False\n",
    "    gene_parts = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "    if gene_parts and \"api\" in gene_parts[0].lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for fasta_fp in src_dir.glob(\"*.fasta\"):\n",
    "    counts = filter_counts(fasta_fp)\n",
    "    print(f\"\\n=== {fasta_fp.name} ===\")\n",
    "    print(f\"  total sequences: {counts['total']}\")\n",
    "    print(f\"  after step1 (non-pseudo): {counts['step1_non_pseudo']}\")\n",
    "    print(f\"  after step2 (isoform 1): {counts['step2_iso1']}\")\n",
    "    print(f\"  after step4 (no keyword): {counts['step4_no_keyword']}\")\n",
    "    print(f\"  after step5 (no API): {counts['step5_no_api']}\\n\")\n",
    "\n",
    "    kept = [rec for rec in SeqIO.parse(fasta_fp, \"fasta\") if keep_record(rec)]\n",
    "\n",
    "    out_fa = out_dir / fasta_fp.name\n",
    "    SeqIO.write(kept, out_fa, \"fasta\")\n",
    "    print(f\"Wrote {len(kept)} records ‚Üí {out_fa.name}\")\n",
    "\n",
    "    gene_ids = set()\n",
    "    for rec in kept:\n",
    "        tok = [p for p in rec.description.split(\"|\") if p.strip().startswith(\"transcript=\")]\n",
    "        if tok:\n",
    "            gene_ids.add(tok[0].split(\"=\", 1)[1].split()[0])\n",
    "    if gene_ids:\n",
    "        txt_out = out_fa.with_suffix(\".geneIDs.txt\")\n",
    "        txt_out.write_text(\"\\n\".join(sorted(gene_ids)))\n",
    "        print(f\"  ‚Üí dumped {len(gene_ids)} gene IDs to {txt_out.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82145ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parent_gaf_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "raw_dir        = Path(\"genomes_to_annotate_with_PlasmoFP\")  # unfiltered FASTAs\n",
    "filtered_dir   = Path(\"genomes_to_annotate_with_PlasmoFP/2_multi_step_filtered_fastas\")\n",
    "embed_dir      = Path(\"2_multi_step_filtered_fastas_embeddings_output\")\n",
    "gaf_out_dir    = Path(\"genomes_to_annotate_with_PlasmoFP/gaf_out_complete_and_filtered_2\")\n",
    "dict_out_dir   = Path(\"genomes_to_annotate_with_PlasmoFP/gene_dicts_out_complete_and_filtered_2\")\n",
    "\n",
    "print(\"Raw FASTA dir :\", raw_dir)\n",
    "print(\"Filtered dir  :\", filtered_dir)\n",
    "print(\"Found filtered FASTAs:\", [p.name for p in filtered_dir.glob(\"*.fasta\")])\n",
    "\n",
    "# ensure outputs exist\n",
    "gaf_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "dict_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# copy only the *_GO.gaf.gz from PlasmoDB into gaf_out_dir\n",
    "for g in parent_gaf_dir.glob(\"*_GO.gaf.gz\"):\n",
    "    shutil.copy2(g, gaf_out_dir / g.name)\n",
    "\n",
    "# mapping from GAF aspect code to human‚Äêreadable\n",
    "aspect_map = {\"c\":\"Component\",\"f\":\"Function\",\"p\":\"Process\"}\n",
    "\n",
    "# your filter helper (unchanged)\n",
    "keywords = [\n",
    "    \"emp1\",\"merozoite surface protein\",\"msp\",\"rifin\",\"stevor\",\"fragment\",\n",
    "    \"vir\",\"variant interspersed repeat\",\"variant interspersed repeats\",\n",
    "    \"fam-\",\"pir\",\"cir\",\"yir\",\"surf\"\n",
    "]\n",
    "\n",
    "def passes_filters(rec):\n",
    "    desc = rec.description.lower()\n",
    "    if \"is_pseudo=false\" not in desc: return False\n",
    "    if \"transcript=\" not in desc:      return False\n",
    "    tid = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "    if not (tid.endswith(\".1\") or tid.endswith(\"_1\") or tid.endswith(\"_t1\")): return False\n",
    "    if any(kw in desc for kw in keywords): return False\n",
    "    gp = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "    if gp and \"api\" in gp[0].lower(): return False\n",
    "    return True\n",
    "\n",
    "def safe_load_embeddings(embed_dir, embedding_stem):\n",
    "    \"\"\"Load embeddings from NPZ, pickle, or numpy file with proper error handling\"\"\"\n",
    "    # Try NPZ first (most reliable)\n",
    "    npz_fp = embed_dir / f\"{embedding_stem}_embeddings.npz\"\n",
    "    if npz_fp.exists():\n",
    "        try:\n",
    "            npz_data = np.load(npz_fp)\n",
    "            embedding_dict = {key: npz_data[key] for key in npz_data.files}\n",
    "            print(f\"   ‚Ä¢ loaded NPZ embeddings for {len(embedding_dict)} sequences\")\n",
    "            return embedding_dict, \"npz\"\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error loading NPZ: {e}\")\n",
    "    \n",
    "    # Try pickle (direct ID mapping)\n",
    "    pkl_fp = embed_dir / f\"{embedding_stem}_embeddings.pkl\"\n",
    "    if pkl_fp.exists():\n",
    "        try:\n",
    "            with open(pkl_fp, \"rb\") as f:\n",
    "                embedding_dict = pickle.load(f)\n",
    "            print(f\"   ‚Ä¢ loaded pickle embeddings for {len(embedding_dict)} sequences\")\n",
    "            return embedding_dict, \"pickle\"\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error loading pickle: {e}\")\n",
    "    \n",
    "    # Fallback to numpy array (requires order-based alignment)\n",
    "    npy_fp = embed_dir / f\"{embedding_stem}_embeddings.npy\"\n",
    "    if npy_fp.exists():\n",
    "        try:\n",
    "            emb_array = np.load(npy_fp)\n",
    "            print(f\"   ‚Ä¢ loaded numpy array embeddings: {emb_array.shape}\")\n",
    "            return emb_array, \"numpy\"\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error loading numpy: {e}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# iterate over each filtered FASTA\n",
    "for filtered_fp in sorted(filtered_dir.glob(\"*.fasta\")):\n",
    "    full_stem = filtered_fp.stem\n",
    "    base      = full_stem.rsplit(\"_AnnotatedProteins\", 1)[0]\n",
    "    print(f\"\\n‚ñ∂Ô∏è  Processing {base}\")\n",
    "\n",
    "    # 1) load embeddings using the safe loader\n",
    "    embedding_stem = full_stem\n",
    "    embeddings_data, embedding_format = safe_load_embeddings(embed_dir, embedding_stem)\n",
    "    \n",
    "    if embeddings_data is None:\n",
    "        print(f\"  ‚ö†Ô∏è  no embeddings found for {embedding_stem}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # 2) load filtered sequences\n",
    "    seqs_filt = list(SeqIO.parse(str(filtered_fp), \"fasta\"))\n",
    "    print(f\"   ‚Ä¢ filtered seqs: {len(seqs_filt)}\")\n",
    "\n",
    "    # 3) build embedding dictionary based on format\n",
    "    if embedding_format in [\"npz\", \"pickle\"]:\n",
    "        # Direct ID-to-embedding mapping\n",
    "        embedding_dict = embeddings_data\n",
    "        print(f\"   ‚Ä¢ using direct ID mapping from {embedding_format}\")\n",
    "        \n",
    "        # Verify coverage\n",
    "        missing_embeddings = 0\n",
    "        for rec in seqs_filt:\n",
    "            if rec.id not in embedding_dict:\n",
    "                missing_embeddings += 1\n",
    "        \n",
    "        if missing_embeddings > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  {missing_embeddings} sequences missing embeddings\")\n",
    "        \n",
    "    elif embedding_format == \"numpy\":\n",
    "        # Order-based alignment (less reliable)\n",
    "        emb_array = embeddings_data\n",
    "        print(f\"   ‚Ä¢ using order-based alignment from numpy array\")\n",
    "        \n",
    "        if len(seqs_filt) != emb_array.shape[0]:\n",
    "            print(f\"  ‚ùå  Count mismatch: {len(seqs_filt)} seqs vs {emb_array.shape[0]} embeddings\")\n",
    "            continue\n",
    "        \n",
    "        # Build dictionary using order\n",
    "        embedding_dict = {}\n",
    "        for idx, rec in enumerate(seqs_filt):\n",
    "            embedding_dict[rec.id] = emb_array[idx]\n",
    "\n",
    "    # 4) load raw sequences\n",
    "    raw_fp = raw_dir / f\"{base}_AnnotatedProteins.fasta\"\n",
    "    if not raw_fp.exists():\n",
    "        print(f\"  ‚ö†Ô∏è  raw FASTA not found: {raw_fp.name}, skipping\")\n",
    "        continue\n",
    "    seqs_raw = list(SeqIO.parse(str(raw_fp), \"fasta\"))\n",
    "\n",
    "    print(f\"   ‚Ä¢ raw seqs: {len(seqs_raw)}    embedding_dict: {len(embedding_dict)}\")\n",
    "\n",
    "    # 5) build gene_dict_complete from the raw seqs WITH embeddings where available\n",
    "    gene_dict_complete = {}\n",
    "    for rec in seqs_raw:\n",
    "        # Direct lookup by sequence ID (most reliable)\n",
    "        emb_vec = embedding_dict.get(rec.id)  # None if not found\n",
    "        \n",
    "        gene_dict_complete[rec.id] = {\n",
    "            \"embedding\": emb_vec,  # Will be None for sequences without embeddings\n",
    "            **{k: [] for k in [\n",
    "                \"GO Component\",\"GO Function\",\"GO Process\",\n",
    "                \"GO IEA Component\",\"GO IEA Function\",\"GO IEA Process\"\n",
    "            ]}\n",
    "        }\n",
    "\n",
    "    # 6) merge GAF annotations into gene_dict_complete - CORRECTED VERSION\n",
    "    gaf_fp = gaf_out_dir / f\"{base}_GO.gaf.gz\"\n",
    "    if not gaf_fp.exists():\n",
    "        print(f\"  ‚ö†Ô∏è  GAF missing for {base}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Build mapping from gene ID to protein IDs (NEW APPROACH)\n",
    "    gene_to_protein_ids = {}\n",
    "    proteins_with_gene_ids = 0\n",
    "    \n",
    "    for rec in seqs_raw:\n",
    "        desc = rec.description\n",
    "        if \"gene=\" in desc:\n",
    "            try:\n",
    "                # Extract gene ID from FASTA description\n",
    "                gene_id = desc.split(\"gene=\")[1].split()[0]\n",
    "                if gene_id not in gene_to_protein_ids:\n",
    "                    gene_to_protein_ids[gene_id] = []\n",
    "                gene_to_protein_ids[gene_id].append(rec.id)\n",
    "                proteins_with_gene_ids += 1\n",
    "            except (IndexError, AttributeError):\n",
    "                # Skip if gene ID extraction fails\n",
    "                continue\n",
    "    \n",
    "    print(f\"   ‚Ä¢ proteins with gene IDs: {proteins_with_gene_ids}\")\n",
    "    print(f\"   ‚Ä¢ unique gene IDs: {len(gene_to_protein_ids)}\")\n",
    "\n",
    "    # Process GAF file with improved matching\n",
    "    gaf_annotations_added = 0\n",
    "    gaf_lines_processed = 0\n",
    "    \n",
    "    with gzip.open(gaf_fp, \"rt\") as gf:\n",
    "        for line in gf:\n",
    "            if line.startswith(\"!\"): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                cols = line.rstrip().split(\"\\t\")\n",
    "                if len(cols) < 9:  # Ensure we have enough columns\n",
    "                    continue\n",
    "                    \n",
    "                gid, term, asp, ev = cols[1], cols[4], cols[8].lower(), cols[6]\n",
    "                key = (\"GO IEA \" if ev == \"IEA\" else \"GO \") + aspect_map[asp]\n",
    "                \n",
    "                # Use proper gene ID matching instead of substring search\n",
    "                if gid in gene_to_protein_ids:\n",
    "                    for protein_id in gene_to_protein_ids[gid]:\n",
    "                        if protein_id in gene_dict_complete:\n",
    "                            gene_dict_complete[protein_id][key].append(term)\n",
    "                            gaf_annotations_added += 1\n",
    "                \n",
    "                gaf_lines_processed += 1\n",
    "                \n",
    "            except (IndexError, KeyError, ValueError):\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "    \n",
    "    print(f\"   ‚Ä¢ GAF lines processed: {gaf_lines_processed}\")\n",
    "    print(f\"   ‚Ä¢ annotations added: {gaf_annotations_added}\")\n",
    "\n",
    "    # 7) build gene_dict_filtered: subset of complete (embeddings already attached)\n",
    "    gene_dict_filtered = {}\n",
    "    for rec in seqs_filt:\n",
    "        if passes_filters(rec):\n",
    "            if rec.id in gene_dict_complete:\n",
    "                gene_dict_filtered[rec.id] = gene_dict_complete[rec.id].copy()\n",
    "\n",
    "    # 8) save both dictionaries\n",
    "    full_out = dict_out_dir / f\"{base}_gene_dict_complete.pkl\"\n",
    "    filt_out = dict_out_dir / f\"{base}_gene_dict_filtered.pkl\"\n",
    "    \n",
    "    with open(full_out, \"wb\") as f:\n",
    "        pickle.dump(gene_dict_complete, f)\n",
    "    with open(filt_out, \"wb\") as f:\n",
    "        pickle.dump(gene_dict_filtered, f)\n",
    "\n",
    "    # Count statistics\n",
    "    complete_with_emb = sum(1 for entry in gene_dict_complete.values() if entry[\"embedding\"] is not None)\n",
    "    filtered_with_emb = sum(1 for entry in gene_dict_filtered.values() if entry[\"embedding\"] is not None)\n",
    "    \n",
    "    # Count GO annotations\n",
    "    complete_with_go = sum(1 for entry in gene_dict_complete.values() \n",
    "                          if any(len(entry[k]) > 0 for k in [\"GO Component\", \"GO Function\", \"GO Process\",\n",
    "                                                            \"GO IEA Component\", \"GO IEA Function\", \"GO IEA Process\"]))\n",
    "    filtered_with_go = sum(1 for entry in gene_dict_filtered.values() \n",
    "                          if any(len(entry[k]) > 0 for k in [\"GO Component\", \"GO Function\", \"GO Process\",\n",
    "                                                            \"GO IEA Component\", \"GO IEA Function\", \"GO IEA Process\"]))\n",
    "    \n",
    "    print(f\"‚úÖ {base}: complete={len(gene_dict_complete)} ({complete_with_emb} with embeddings, {complete_with_go} with GO)\")\n",
    "    print(f\"   filtered={len(gene_dict_filtered)} ({filtered_with_emb} with embeddings, {filtered_with_go} with GO)\")\n",
    "    print(f\"   ‚Ä¢ embedding format used: {embedding_format}\")\n",
    "\n",
    "print(\"\\nüéâ Gene dictionary generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e14f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_annotation_completeness(dict_dir):\n",
    "    \"\"\"\n",
    "    Analyze GO annotation completeness across all gene dictionaries.\n",
    "    \n",
    "    Returns DataFrame with counts of:\n",
    "    - Unannotated: No GO annotations in any subontology\n",
    "    - Partially annotated: At least 1 annotation in any subontology \n",
    "    - Fully annotated: Annotations in all 3 subontologies (MF, BP, CC)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all gene dictionary files\n",
    "    dict_files = list(dict_dir.glob(\"*_gene_dict_complete.pkl\"))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    GO_KEYS = [\n",
    "        (\"GO Function\", \"GO IEA Function\"),      # MF\n",
    "        (\"GO Process\", \"GO IEA Process\"),        # BP  \n",
    "        (\"GO Component\", \"GO IEA Component\"),    # CC\n",
    "    ]\n",
    "    \n",
    "    for dict_file in sorted(dict_files):\n",
    "        # Extract species name\n",
    "        species = dict_file.stem.replace(\"_gene_dict_complete\", \"\")\n",
    "        \n",
    "        # Load dictionary\n",
    "        with open(dict_file, \"rb\") as f:\n",
    "            gene_dict = pickle.load(f)\n",
    "        \n",
    "        # Initialize counters\n",
    "        unannotated = 0\n",
    "        partially_annotated = 0\n",
    "        fully_annotated = 0\n",
    "        \n",
    "        # Analyze each protein\n",
    "        for protein_id, protein_data in gene_dict.items():\n",
    "            # Check annotation status for each subontology\n",
    "            subontology_counts = []\n",
    "            \n",
    "            for go_key, go_iea_key in GO_KEYS:\n",
    "                # Count annotations in this subontology (combining GO and IEA)\n",
    "                go_terms = protein_data.get(go_key, [])\n",
    "                go_iea_terms = protein_data.get(go_iea_key, [])\n",
    "                \n",
    "                # Handle different data types (list, set, or single values)\n",
    "                if isinstance(go_terms, str):\n",
    "                    go_set = {go_terms} if go_terms else set()\n",
    "                elif hasattr(go_terms, '__iter__'):\n",
    "                    go_set = set(go_terms)\n",
    "                else:\n",
    "                    go_set = set()\n",
    "                \n",
    "                if isinstance(go_iea_terms, str):\n",
    "                    go_iea_set = {go_iea_terms} if go_iea_terms else set()\n",
    "                elif hasattr(go_iea_terms, '__iter__'):\n",
    "                    go_iea_set = set(go_iea_terms)\n",
    "                else:\n",
    "                    go_iea_set = set()\n",
    "                \n",
    "                # Combined annotation count for this subontology\n",
    "                combined_count = len(go_set | go_iea_set)\n",
    "                subontology_counts.append(combined_count)\n",
    "            \n",
    "            # Determine annotation completeness\n",
    "            annotated_subontologies = sum(1 for count in subontology_counts if count > 0)\n",
    "            \n",
    "            if annotated_subontologies == 0:\n",
    "                unannotated += 1\n",
    "            elif annotated_subontologies == 3:\n",
    "                fully_annotated += 1\n",
    "            else:  # 1 or 2 subontologies\n",
    "                partially_annotated += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_proteins = len(gene_dict)\n",
    "        unannotated_pct = (unannotated / total_proteins) * 100\n",
    "        partially_pct = (partially_annotated / total_proteins) * 100\n",
    "        fully_pct = (fully_annotated / total_proteins) * 100\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Species': species,\n",
    "            'Total_Proteins': total_proteins,\n",
    "            'Unannotated_Count': unannotated,\n",
    "            'Partially_Annotated_Count': partially_annotated,\n",
    "            'Fully_Annotated_Count': fully_annotated,\n",
    "            'Unannotated_Percent': round(unannotated_pct, 2),\n",
    "            'Partially_Annotated_Percent': round(partially_pct, 2),\n",
    "            'Fully_Annotated_Percent': round(fully_pct, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ {species}: {total_proteins} proteins\")\n",
    "        print(f\"   Unannotated: {unannotated} ({unannotated_pct:.1f}%)\")\n",
    "        print(f\"   Partial: {partially_annotated} ({partially_pct:.1f}%)\")\n",
    "        print(f\"   Full: {fully_annotated} ({fully_pct:.1f}%)\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(f\"\\nüìä SUMMARY ACROSS ALL SPECIES:\")\n",
    "    print(f\"Total species analyzed: {len(df)}\")\n",
    "    print(f\"Average unannotated: {df['Unannotated_Percent'].mean():.1f}%\")\n",
    "    print(f\"Average partial: {df['Partially_Annotated_Percent'].mean():.1f}%\")\n",
    "    print(f\"Average full: {df['Fully_Annotated_Percent'].mean():.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detailed_subontology_analysis(dict_dir):\n",
    "    \"\"\"\n",
    "    More detailed analysis breaking down by individual subontologies.\n",
    "    \n",
    "    Returns DataFrame with annotation counts for MF, BP, CC individually.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_files = list(dict_dir.glob(\"*_gene_dict_complete.pkl\"))\n",
    "    results = []\n",
    "    \n",
    "    GO_KEYS = {\n",
    "        'MF': (\"GO Function\", \"GO IEA Function\"),\n",
    "        'BP': (\"GO Process\", \"GO IEA Process\"), \n",
    "        'CC': (\"GO Component\", \"GO IEA Component\")\n",
    "    }\n",
    "    \n",
    "    for dict_file in sorted(dict_files):\n",
    "        species = dict_file.stem.replace(\"_gene_dict_complete\", \"\")\n",
    "        \n",
    "        with open(dict_file, \"rb\") as f:\n",
    "            gene_dict = pickle.load(f)\n",
    "        \n",
    "        # Count annotations per subontology\n",
    "        subontology_counts = {onto: 0 for onto in GO_KEYS.keys()}\n",
    "        \n",
    "        for protein_id, protein_data in gene_dict.items():\n",
    "            for onto, (go_key, go_iea_key) in GO_KEYS.items():\n",
    "                go_terms = set(protein_data.get(go_key, []))\n",
    "                go_iea_terms = set(protein_data.get(go_iea_key, []))\n",
    "                \n",
    "                # Remove empty strings/None\n",
    "                go_terms.discard('')\n",
    "                go_terms.discard(None)\n",
    "                go_iea_terms.discard('')\n",
    "                go_iea_terms.discard(None)\n",
    "                \n",
    "                if len(go_terms | go_iea_terms) > 0:\n",
    "                    subontology_counts[onto] += 1\n",
    "        \n",
    "        total = len(gene_dict)\n",
    "        result = {\n",
    "            'Species': species,\n",
    "            'Total_Proteins': total,\n",
    "            'MF_Annotated': subontology_counts['MF'],\n",
    "            'BP_Annotated': subontology_counts['BP'],\n",
    "            'CC_Annotated': subontology_counts['CC'],\n",
    "            'MF_Percent': round((subontology_counts['MF'] / total) * 100, 2),\n",
    "            'BP_Percent': round((subontology_counts['BP'] / total) * 100, 2),\n",
    "            'CC_Percent': round((subontology_counts['CC'] / total) * 100, 2)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set path to gene dictionaries\n",
    "    dict_dir = Path(\"genomes_to_annotate_with_PlasmoFP/gene_dicts_out_complete_and_filtered_2\")\n",
    "    \n",
    "    # Generate baseline annotation completeness analysis\n",
    "    print(\"üî¨ BASELINE GO ANNOTATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    baseline_df = analyze_annotation_completeness(dict_dir)\n",
    "    \n",
    "    # Save results\n",
    "    baseline_df.to_csv(\"baseline_annotation_completeness.csv\", index=False)\n",
    "    print(f\"\\nüíæ Saved baseline analysis to: baseline_annotation_completeness.csv\")\n",
    "    \n",
    "    # Generate detailed subontology analysis\n",
    "    print(f\"\\nüîç DETAILED SUBONTOLOGY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    detailed_df = detailed_subontology_analysis(dict_dir)\n",
    "    detailed_df.to_csv(\"detailed_subontology_analysis.csv\", index=False)\n",
    "    print(f\"\\nüíæ Saved detailed analysis to: detailed_subontology_analysis.csv\")\n",
    "    \n",
    "    # Display summary table\n",
    "    print(f\"\\nüìã BASELINE COMPLETENESS SUMMARY:\")\n",
    "    print(baseline_df[['Species', 'Total_Proteins', 'Unannotated_Percent', \n",
    "                      'Partially_Annotated_Percent', 'Fully_Annotated_Percent']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
