{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c52cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PbergheiANKA', 'PblacklockiG01', 'PmalariaeUG01', 'PvinckeibrucechwattiDA', 'PreichenowiCDC', 'PcynomolgiM', 'PovalecurtisiGH01', 'PovalewallikeriPowCR01', 'Pfalciparum3D7', 'PgaboniG01', 'PknowlesiH', 'PadleriG01', 'Pgallinaceum8A', 'PvivaxSal1', 'Pchabaudichabaudi', 'PinuiSanAntonio1', 'PfragileNilgiri', 'PcoatneyiHackeri', 'Pyoeliiyoelii17XNL2023'}\n",
      "['PadleriG01', 'PbergheiANKA', 'PblacklockiG01', 'Pchabaudichabaudi', 'PcoatneyiHackeri', 'PcynomolgiM', 'Pfalciparum3D7', 'PfragileNilgiri', 'PgaboniG01', 'Pgallinaceum8A', 'PinuiSanAntonio1', 'PknowlesiH', 'PmalariaeUG01', 'PovalecurtisiGH01', 'PovalewallikeriPowCR01', 'PreichenowiCDC', 'PvinckeibrucechwattiDA', 'PvivaxSal1', 'Pyoeliiyoelii17XNL2023']\n"
     ]
    }
   ],
   "source": [
    "fasta_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "fasta_files = list(fasta_dir.glob(\"*.fasta\"))\n",
    "\n",
    "genomes = { f.name.split('_')[1] for f in fasta_files }\n",
    "\n",
    "genome_list = sorted(genomes)\n",
    "\n",
    "print(genomes)       # e.g. {'PbergheiANKA', 'PvivaxSal1', ...}\n",
    "print(genome_list)   # e.g. ['PbergheiANKA', 'PblacklockiG01', ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c637f080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(genome_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74784198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "\n",
    "species_to_fasta = {}\n",
    "for sp in genome_list:\n",
    "    fn = f\"PlasmoDB-68_{sp}_AnnotatedProteins.fasta\"\n",
    "    fp = fasta_dir / fn\n",
    "    if fp.exists():\n",
    "        species_to_fasta[sp] = fp\n",
    "    else:\n",
    "        matches = list(fasta_dir.glob(f\"*{sp}*AnnotatedProteins*.fasta\"))\n",
    "        if len(matches) == 1:\n",
    "            species_to_fasta[sp] = matches[0]\n",
    "        elif len(matches) > 1:\n",
    "            print(f\"multiple matches for {sp}: {matches}\")\n",
    "        else:\n",
    "            print(f\"no file found for {sp}\")\n",
    "\n",
    "for sp, fasta_path in species_to_fasta.items():\n",
    "    print(f\"{sp} → {fasta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = Path(\"genomes_to_annotate_with_PlasmoFP/annotated_fasta\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for species, fasta_path in species_to_fasta.items():\n",
    "    dest_path = dest_dir / fasta_path.name\n",
    "    try:\n",
    "        shutil.copy2(fasta_path, dest_path)\n",
    "        print(f\"{species}: copied to {dest_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{species}: source file not found ({fasta_path})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{species}: failed to copy ({e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path(\"genomes_to_annotate_with_PlasmoFP/annotated_fasta\")\n",
    "out_dir = Path(\"genomes_to_annotate_with_PlasmoFP/1_length_filtered_fastas\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fasta_fp in src_dir.glob(\"*.fasta\"):\n",
    "    filtered_records = (\n",
    "        rec for rec in SeqIO.parse(fasta_fp, \"fasta\")\n",
    "        if len(rec.seq) <= 1200\n",
    "    )\n",
    "    out_fp = out_dir / (fasta_fp.stem + \"_filtered.fasta\")\n",
    "    count = SeqIO.write(filtered_records, out_fp, \"fasta\")\n",
    "    print(f\"{fasta_fp.name}: wrote {count} records ≤1200 aa → {out_fp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c95b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path(\"genomes_to_annotate_with_PlasmoFP/1_length_filtered_fastas\")\n",
    "out_dir = Path(\"genomes_to_annotate_with_PlasmoFP/2_multi_step_filtered_fastas\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "keywords = [\n",
    "    \"emp1\", \"merozoite surface protein\", \"msp\",\n",
    "    \"rifin\", \"stevor\", \"fragment\",\n",
    "    \"vir\", \"variant interspersed repeat\", \"variant interspersed repeats\",\n",
    "    \"fam-\",    # fam-a/b/c...\n",
    "    \"pir\",     # PIR/CIR\n",
    "    \"cir\",\n",
    "    \"yir\",     # YIR family\n",
    "    \"surf\",    # SURFIN\n",
    "]\n",
    "\n",
    "def filter_counts(fasta_fp):\n",
    "    counts = {\n",
    "        \"total\": 0,\n",
    "        \"step1_non_pseudo\": 0,\n",
    "        \"step2_iso1\": 0,\n",
    "        \"step4_no_keyword\": 0,\n",
    "        \"step5_no_api\": 0,\n",
    "    }\n",
    "    for rec in SeqIO.parse(fasta_fp, \"fasta\"):\n",
    "        counts[\"total\"] += 1\n",
    "        desc = rec.description.lower()\n",
    "\n",
    "        if \"is_pseudo=false\" not in desc:\n",
    "            continue\n",
    "        counts[\"step1_non_pseudo\"] += 1\n",
    "\n",
    "        if \"transcript=\" not in desc:\n",
    "            continue\n",
    "        trans_id = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "        if not (trans_id.endswith(\".1\") or trans_id.endswith(\"_1\") or trans_id.endswith(\"_t1\")):\n",
    "            continue\n",
    "        counts[\"step2_iso1\"] += 1\n",
    "\n",
    "        if any(kw in desc for kw in keywords):\n",
    "            continue\n",
    "        counts[\"step4_no_keyword\"] += 1\n",
    "\n",
    "        gene_parts = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "        if gene_parts and \"api\" in gene_parts[0].lower():\n",
    "            continue\n",
    "        counts[\"step5_no_api\"] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def keep_record(rec):\n",
    "    desc = rec.description.lower()\n",
    "    if \"is_pseudo=false\" not in desc:\n",
    "        return False\n",
    "    if \"transcript=\" not in desc:\n",
    "        return False\n",
    "    trans_id = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "    if not (trans_id.endswith(\".1\") or trans_id.endswith(\"_1\") or trans_id.endswith(\"_t1\")):\n",
    "        return False\n",
    "    if any(kw in desc for kw in keywords):\n",
    "        return False\n",
    "    gene_parts = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "    if gene_parts and \"api\" in gene_parts[0].lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for fasta_fp in src_dir.glob(\"*.fasta\"):\n",
    "    counts = filter_counts(fasta_fp)\n",
    "    print(f\"\\n=== {fasta_fp.name} ===\")\n",
    "    print(f\"  total sequences: {counts['total']}\")\n",
    "    print(f\"  after step1 (non-pseudo): {counts['step1_non_pseudo']}\")\n",
    "    print(f\"  after step2 (isoform 1): {counts['step2_iso1']}\")\n",
    "    print(f\"  after step4 (no keyword): {counts['step4_no_keyword']}\")\n",
    "    print(f\"  after step5 (no API): {counts['step5_no_api']}\\n\")\n",
    "\n",
    "    kept = [rec for rec in SeqIO.parse(fasta_fp, \"fasta\") if keep_record(rec)]\n",
    "\n",
    "    out_fa = out_dir / fasta_fp.name\n",
    "    SeqIO.write(kept, out_fa, \"fasta\")\n",
    "    print(f\"Wrote {len(kept)} records → {out_fa.name}\")\n",
    "\n",
    "    gene_ids = set()\n",
    "    for rec in kept:\n",
    "        tok = [p for p in rec.description.split(\"|\") if p.strip().startswith(\"transcript=\")]\n",
    "        if tok:\n",
    "            gene_ids.add(tok[0].split(\"=\", 1)[1].split()[0])\n",
    "    if gene_ids:\n",
    "        txt_out = out_fa.with_suffix(\".geneIDs.txt\")\n",
    "        txt_out.write_text(\"\\n\".join(sorted(gene_ids)))\n",
    "        print(f\"  → dumped {len(gene_ids)} gene IDs to {txt_out.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82145ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_gaf_dir = Path(\"genomes_to_annotate_with_PlasmoFP\")\n",
    "raw_dir        = Path(\"genomes_to_annotate_with_PlasmoFP\")  # unfiltered FASTAs\n",
    "filtered_dir   = Path(\"genomes_to_annotate_with_PlasmoFP/2_multi_step_filtered_fastas\")\n",
    "embed_dir      = Path(\"2_multi_step_filtered_fastas_embeddings_output\")\n",
    "gaf_out_dir    = Path(\"genomes_to_annotate_with_PlasmoFP/gaf_out_complete_and_filtered_2\")\n",
    "dict_out_dir   = Path(\"genomes_to_annotate_with_PlasmoFP/gene_dicts_out_complete_and_filtered_2\")\n",
    "\n",
    "print(\"Raw FASTA dir :\", raw_dir)\n",
    "print(\"Filtered dir  :\", filtered_dir)\n",
    "print(\"Found filtered FASTAs:\", [p.name for p in filtered_dir.glob(\"*.fasta\")])\n",
    "\n",
    "gaf_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "dict_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for g in parent_gaf_dir.glob(\"*_GO.gaf.gz\"):\n",
    "    shutil.copy2(g, gaf_out_dir / g.name)\n",
    "\n",
    "aspect_map = {\"c\":\"Component\",\"f\":\"Function\",\"p\":\"Process\"}\n",
    "\n",
    "keywords = [\n",
    "    \"emp1\",\"merozoite surface protein\",\"msp\",\"rifin\",\"stevor\",\"fragment\",\n",
    "    \"vir\",\"variant interspersed repeat\",\"variant interspersed repeats\",\n",
    "    \"fam-\",\"pir\",\"cir\",\"yir\",\"surf\"\n",
    "]\n",
    "\n",
    "def passes_filters(rec):\n",
    "    desc = rec.description.lower()\n",
    "    if \"is_pseudo=false\" not in desc: return False\n",
    "    if \"transcript=\" not in desc:      return False\n",
    "    tid = desc.split(\"transcript=\")[1].split()[0].lower()\n",
    "    if not (tid.endswith(\".1\") or tid.endswith(\"_1\") or tid.endswith(\"_t1\")): return False\n",
    "    if any(kw in desc for kw in keywords): return False\n",
    "    gp = [p for p in desc.split(\"|\") if p.strip().startswith(\"gene=\")]\n",
    "    if gp and \"api\" in gp[0].lower(): return False\n",
    "    return True\n",
    "\n",
    "def safe_load_embeddings(embed_dir, embedding_stem):\n",
    "    \"\"\"Load embeddings from NPZ, pickle, or numpy file with proper error handling\"\"\"\n",
    "    npz_fp = embed_dir / f\"{embedding_stem}_embeddings.npz\"\n",
    "    if npz_fp.exists():\n",
    "        try:\n",
    "            npz_data = np.load(npz_fp)\n",
    "            embedding_dict = {key: npz_data[key] for key in npz_data.files}\n",
    "            print(f\"   • loaded NPZ embeddings for {len(embedding_dict)} sequences\")\n",
    "            return embedding_dict, \"npz\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NPZ: {e}\")\n",
    "    \n",
    "    pkl_fp = embed_dir / f\"{embedding_stem}_embeddings.pkl\"\n",
    "    if pkl_fp.exists():\n",
    "        try:\n",
    "            with open(pkl_fp, \"rb\") as f:\n",
    "                embedding_dict = pickle.load(f)\n",
    "            print(f\"   • loaded pickle embeddings for {len(embedding_dict)} sequences\")\n",
    "            return embedding_dict, \"pickle\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle: {e}\")\n",
    "    \n",
    "    npy_fp = embed_dir / f\"{embedding_stem}_embeddings.npy\"\n",
    "    if npy_fp.exists():\n",
    "        try:\n",
    "            emb_array = np.load(npy_fp)\n",
    "            print(f\"   • loaded numpy array embeddings: {emb_array.shape}\")\n",
    "            return emb_array, \"numpy\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading numpy: {e}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "for filtered_fp in sorted(filtered_dir.glob(\"*.fasta\")):\n",
    "    full_stem = filtered_fp.stem\n",
    "    base      = full_stem.rsplit(\"_AnnotatedProteins\", 1)[0]\n",
    "    print(f\"Processing {base}\")\n",
    "\n",
    "    embedding_stem = full_stem\n",
    "    embeddings_data, embedding_format = safe_load_embeddings(embed_dir, embedding_stem)\n",
    "    \n",
    "    if embeddings_data is None:\n",
    "        print(f\"no embeddings found for {embedding_stem}, skipping\")\n",
    "        continue\n",
    "\n",
    "    seqs_filt = list(SeqIO.parse(str(filtered_fp), \"fasta\"))\n",
    "    print(f\"filtered seqs: {len(seqs_filt)}\")\n",
    "\n",
    "    if embedding_format in [\"npz\", \"pickle\"]:\n",
    "        embedding_dict = embeddings_data\n",
    "        print(f\"using direct ID mapping from {embedding_format}\")\n",
    "        \n",
    "        missing_embeddings = 0\n",
    "        for rec in seqs_filt:\n",
    "            if rec.id not in embedding_dict:\n",
    "                missing_embeddings += 1\n",
    "        \n",
    "        if missing_embeddings > 0:\n",
    "            print(f\"{missing_embeddings} sequences missing embeddings\")\n",
    "        \n",
    "    elif embedding_format == \"numpy\":\n",
    "        emb_array = embeddings_data\n",
    "        print(f\"using order-based alignment from numpy array\")\n",
    "        \n",
    "        if len(seqs_filt) != emb_array.shape[0]:\n",
    "            print(f\"Count mismatch: {len(seqs_filt)} seqs vs {emb_array.shape[0]} embeddings\")\n",
    "            continue\n",
    "        \n",
    "        embedding_dict = {}\n",
    "        for idx, rec in enumerate(seqs_filt):\n",
    "            embedding_dict[rec.id] = emb_array[idx]\n",
    "\n",
    "    raw_fp = raw_dir / f\"{base}_AnnotatedProteins.fasta\"\n",
    "    if not raw_fp.exists():\n",
    "        print(f\"raw FASTA not found: {raw_fp.name}, skipping\")\n",
    "        continue\n",
    "    seqs_raw = list(SeqIO.parse(str(raw_fp), \"fasta\"))\n",
    "\n",
    "    print(f\"raw seqs: {len(seqs_raw)}    embedding_dict: {len(embedding_dict)}\")\n",
    "\n",
    "    gene_dict_complete = {}\n",
    "    for rec in seqs_raw:\n",
    "        emb_vec = embedding_dict.get(rec.id)  # None if not found\n",
    "        \n",
    "        gene_dict_complete[rec.id] = {\n",
    "            \"embedding\": emb_vec,  # Will be None for sequences without embeddings\n",
    "            **{k: [] for k in [\n",
    "                \"GO Component\",\"GO Function\",\"GO Process\",\n",
    "                \"GO IEA Component\",\"GO IEA Function\",\"GO IEA Process\"\n",
    "            ]}\n",
    "        }\n",
    "\n",
    "    gaf_fp = gaf_out_dir / f\"{base}_GO.gaf.gz\"\n",
    "    if not gaf_fp.exists():\n",
    "        print(f\"GAF missing for {base}, skipping\")\n",
    "        continue\n",
    "\n",
    "    gene_to_protein_ids = {}\n",
    "    proteins_with_gene_ids = 0\n",
    "    \n",
    "    for rec in seqs_raw:\n",
    "        desc = rec.description\n",
    "        if \"gene=\" in desc:\n",
    "            try:\n",
    "                gene_id = desc.split(\"gene=\")[1].split()[0]\n",
    "                if gene_id not in gene_to_protein_ids:\n",
    "                    gene_to_protein_ids[gene_id] = []\n",
    "                gene_to_protein_ids[gene_id].append(rec.id)\n",
    "                proteins_with_gene_ids += 1\n",
    "            except (IndexError, AttributeError):\n",
    "                continue\n",
    "    \n",
    "    print(f\"proteins with gene IDs: {proteins_with_gene_ids}\")\n",
    "    print(f\"unique gene IDs: {len(gene_to_protein_ids)}\")\n",
    "\n",
    "    gaf_annotations_added = 0\n",
    "    gaf_lines_processed = 0\n",
    "    \n",
    "    with gzip.open(gaf_fp, \"rt\") as gf:\n",
    "        for line in gf:\n",
    "            if line.startswith(\"!\"): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                cols = line.rstrip().split(\"\\t\")\n",
    "                if len(cols) < 9:  \n",
    "                    continue\n",
    "                    \n",
    "                gid, term, asp, ev = cols[1], cols[4], cols[8].lower(), cols[6]\n",
    "                key = (\"GO IEA \" if ev == \"IEA\" else \"GO \") + aspect_map[asp]\n",
    "                \n",
    "                if gid in gene_to_protein_ids:\n",
    "                    for protein_id in gene_to_protein_ids[gid]:\n",
    "                        if protein_id in gene_dict_complete:\n",
    "                            gene_dict_complete[protein_id][key].append(term)\n",
    "                            gaf_annotations_added += 1\n",
    "                \n",
    "                gaf_lines_processed += 1\n",
    "                \n",
    "            except (IndexError, KeyError, ValueError):\n",
    "                continue\n",
    "    \n",
    "    print(f\"GAF lines processed: {gaf_lines_processed}\")\n",
    "    print(f\"annotations added: {gaf_annotations_added}\")\n",
    "\n",
    "    gene_dict_filtered = {}\n",
    "    for rec in seqs_filt:\n",
    "        if passes_filters(rec):\n",
    "            if rec.id in gene_dict_complete:\n",
    "                gene_dict_filtered[rec.id] = gene_dict_complete[rec.id].copy()\n",
    "\n",
    "    full_out = dict_out_dir / f\"{base}_gene_dict_complete.pkl\"\n",
    "    filt_out = dict_out_dir / f\"{base}_gene_dict_filtered.pkl\"\n",
    "    \n",
    "    with open(full_out, \"wb\") as f:\n",
    "        pickle.dump(gene_dict_complete, f)\n",
    "    with open(filt_out, \"wb\") as f:\n",
    "        pickle.dump(gene_dict_filtered, f)\n",
    "\n",
    "    complete_with_emb = sum(1 for entry in gene_dict_complete.values() if entry[\"embedding\"] is not None)\n",
    "    filtered_with_emb = sum(1 for entry in gene_dict_filtered.values() if entry[\"embedding\"] is not None)\n",
    "    \n",
    "    complete_with_go = sum(1 for entry in gene_dict_complete.values() \n",
    "                          if any(len(entry[k]) > 0 for k in [\"GO Component\", \"GO Function\", \"GO Process\",\n",
    "                                                            \"GO IEA Component\", \"GO IEA Function\", \"GO IEA Process\"]))\n",
    "    filtered_with_go = sum(1 for entry in gene_dict_filtered.values() \n",
    "                          if any(len(entry[k]) > 0 for k in [\"GO Component\", \"GO Function\", \"GO Process\",\n",
    "                                                            \"GO IEA Component\", \"GO IEA Function\", \"GO IEA Process\"]))\n",
    "    \n",
    "    print(f\"{base}: complete={len(gene_dict_complete)} ({complete_with_emb} with embeddings, {complete_with_go} with GO)\")\n",
    "    print(f\"filtered={len(gene_dict_filtered)} ({filtered_with_emb} with embeddings, {filtered_with_go} with GO)\")\n",
    "    print(f\"embedding format used: {embedding_format}\")\n",
    "\n",
    "print(\"\\nGene dictionary generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e14f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_annotation_completeness(dict_dir):\n",
    "    \"\"\"\n",
    "    Analyze GO annotation completeness across all gene dictionaries.\n",
    "    \n",
    "    Returns DataFrame with counts of:\n",
    "    - Unannotated: No GO annotations in any subontology\n",
    "    - Partially annotated: At least 1 annotation in any subontology \n",
    "    - Fully annotated: Annotations in all 3 subontologies (MF, BP, CC)\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_files = list(dict_dir.glob(\"*_gene_dict_complete.pkl\"))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    GO_KEYS = [\n",
    "        (\"GO Function\", \"GO IEA Function\"),      # MF\n",
    "        (\"GO Process\", \"GO IEA Process\"),        # BP  \n",
    "        (\"GO Component\", \"GO IEA Component\"),    # CC\n",
    "    ]\n",
    "    \n",
    "    for dict_file in sorted(dict_files):\n",
    "        species = dict_file.stem.replace(\"_gene_dict_complete\", \"\")\n",
    "        \n",
    "        with open(dict_file, \"rb\") as f:\n",
    "            gene_dict = pickle.load(f)\n",
    "        \n",
    "        unannotated = 0\n",
    "        partially_annotated = 0\n",
    "        fully_annotated = 0\n",
    "        \n",
    "        for protein_id, protein_data in gene_dict.items():\n",
    "            subontology_counts = []\n",
    "            \n",
    "            for go_key, go_iea_key in GO_KEYS:\n",
    "                go_terms = protein_data.get(go_key, [])\n",
    "                go_iea_terms = protein_data.get(go_iea_key, [])\n",
    "                \n",
    "                if isinstance(go_terms, str):\n",
    "                    go_set = {go_terms} if go_terms else set()\n",
    "                elif hasattr(go_terms, '__iter__'):\n",
    "                    go_set = set(go_terms)\n",
    "                else:\n",
    "                    go_set = set()\n",
    "                \n",
    "                if isinstance(go_iea_terms, str):\n",
    "                    go_iea_set = {go_iea_terms} if go_iea_terms else set()\n",
    "                elif hasattr(go_iea_terms, '__iter__'):\n",
    "                    go_iea_set = set(go_iea_terms)\n",
    "                else:\n",
    "                    go_iea_set = set()\n",
    "                \n",
    "                combined_count = len(go_set | go_iea_set)\n",
    "                subontology_counts.append(combined_count)\n",
    "            \n",
    "            annotated_subontologies = sum(1 for count in subontology_counts if count > 0)\n",
    "            \n",
    "            if annotated_subontologies == 0:\n",
    "                unannotated += 1\n",
    "            elif annotated_subontologies == 3:\n",
    "                fully_annotated += 1\n",
    "            else:  \n",
    "                partially_annotated += 1\n",
    "        \n",
    "        total_proteins = len(gene_dict)\n",
    "        unannotated_pct = (unannotated / total_proteins) * 100\n",
    "        partially_pct = (partially_annotated / total_proteins) * 100\n",
    "        fully_pct = (fully_annotated / total_proteins) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'Species': species,\n",
    "            'Total_Proteins': total_proteins,\n",
    "            'Unannotated_Count': unannotated,\n",
    "            'Partially_Annotated_Count': partially_annotated,\n",
    "            'Fully_Annotated_Count': fully_annotated,\n",
    "            'Unannotated_Percent': round(unannotated_pct, 2),\n",
    "            'Partially_Annotated_Percent': round(partially_pct, 2),\n",
    "            'Fully_Annotated_Percent': round(fully_pct, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"{species}: {total_proteins} proteins\")\n",
    "        print(f\"Unannotated: {unannotated} ({unannotated_pct:.1f}%)\")\n",
    "        print(f\"Partial: {partially_annotated} ({partially_pct:.1f}%)\")\n",
    "        print(f\"Full: {fully_annotated} ({fully_pct:.1f}%)\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nSUMMARY ACROSS ALL SPECIES:\")\n",
    "    print(f\"Total species analyzed: {len(df)}\")\n",
    "    print(f\"Average unannotated: {df['Unannotated_Percent'].mean():.1f}%\")\n",
    "    print(f\"Average partial: {df['Partially_Annotated_Percent'].mean():.1f}%\")\n",
    "    print(f\"Average full: {df['Fully_Annotated_Percent'].mean():.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detailed_subontology_analysis(dict_dir):\n",
    "    \"\"\"\n",
    "    Breaking down by individual subontologies.\n",
    "    \n",
    "    Returns DataFrame with annotation counts for MF, BP, CC individually.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_files = list(dict_dir.glob(\"*_gene_dict_complete.pkl\"))\n",
    "    results = []\n",
    "    \n",
    "    GO_KEYS = {\n",
    "        'MF': (\"GO Function\", \"GO IEA Function\"),\n",
    "        'BP': (\"GO Process\", \"GO IEA Process\"), \n",
    "        'CC': (\"GO Component\", \"GO IEA Component\")\n",
    "    }\n",
    "    \n",
    "    for dict_file in sorted(dict_files):\n",
    "        species = dict_file.stem.replace(\"_gene_dict_complete\", \"\")\n",
    "        \n",
    "        with open(dict_file, \"rb\") as f:\n",
    "            gene_dict = pickle.load(f)\n",
    "        \n",
    "        subontology_counts = {onto: 0 for onto in GO_KEYS.keys()}\n",
    "        \n",
    "        for protein_id, protein_data in gene_dict.items():\n",
    "            for onto, (go_key, go_iea_key) in GO_KEYS.items():\n",
    "                go_terms = set(protein_data.get(go_key, []))\n",
    "                go_iea_terms = set(protein_data.get(go_iea_key, []))\n",
    "                \n",
    "                go_terms.discard('')\n",
    "                go_terms.discard(None)\n",
    "                go_iea_terms.discard('')\n",
    "                go_iea_terms.discard(None)\n",
    "                \n",
    "                if len(go_terms | go_iea_terms) > 0:\n",
    "                    subontology_counts[onto] += 1\n",
    "        \n",
    "        total = len(gene_dict)\n",
    "        result = {\n",
    "            'Species': species,\n",
    "            'Total_Proteins': total,\n",
    "            'MF_Annotated': subontology_counts['MF'],\n",
    "            'BP_Annotated': subontology_counts['BP'],\n",
    "            'CC_Annotated': subontology_counts['CC'],\n",
    "            'MF_Percent': round((subontology_counts['MF'] / total) * 100, 2),\n",
    "            'BP_Percent': round((subontology_counts['BP'] / total) * 100, 2),\n",
    "            'CC_Percent': round((subontology_counts['CC'] / total) * 100, 2)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dict_dir = Path(\"genomes_to_annotate_with_PlasmoFP/gene_dicts_out_complete_and_filtered_2\")\n",
    "     \n",
    "    baseline_df = analyze_annotation_completeness(dict_dir)\n",
    "    \n",
    "    baseline_df.to_csv(\"baseline_annotation_completeness.csv\", index=False)\n",
    "    print(f\"Saved baseline analysis to: baseline_annotation_completeness.csv\")\n",
    "        \n",
    "    detailed_df = detailed_subontology_analysis(dict_dir)\n",
    "    detailed_df.to_csv(\"detailed_subontology_analysis.csv\", index=False)\n",
    "    print(f\"Saved detailed analysis to: detailed_subontology_analysis.csv\")\n",
    "    \n",
    "    print(baseline_df[['Species', 'Total_Proteins', 'Unannotated_Percent', \n",
    "                      'Partially_Annotated_Percent', 'Fully_Annotated_Percent']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
